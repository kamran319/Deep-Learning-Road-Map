{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29818afa",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "- The output of the NN will ranges from 0-1 for the sigmoid AF\n",
    "- whenever we are using sigmoid AF if the output is less than 0.5 it will transform to the output to ZERO - Non Active Neuron\n",
    "- whenever we are using sigmoid AF if the output is greater than 0.5 it will transform to the output to ONE - Active Nuron\n",
    "- used in Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70214efd",
   "metadata": {},
   "source": [
    "# RELU\n",
    "- The output ranges btwn -1 to +1\n",
    "- if y is -ve then the output will be always ZERO\n",
    "- if y is +ve then the output will be always ONE, TWO, THREE, FOUR etc\n",
    "- Whenever solving the Regression problem most of the time will be using RELU let be in the hidden layer or output layer\n",
    "- Whenever solving the Classification problem most of the time will be using SIGMOID( coz it can classify records into many categories) for the output nodes, and relu for the hidden nodes\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65424b",
   "metadata": {},
   "source": [
    "# Tanh\n",
    "- It ranges between -1 to +1\n",
    "- derivative of this will be ranges from 0 to 1\n",
    "- It is Zero centred which is better than sigmoid AF\n",
    "- It will result in Vanishing gradient problem as its derivative ranges from 0 to 1\n",
    "- derivative of negative is zero, it will result in dead AF or dying relu\n",
    "- It is used in Hidden layers as it solves Vanishing gradient problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d21006",
   "metadata": {},
   "source": [
    "# Leaky RELU\n",
    "- Will be multiplying 0.01 with x \n",
    "- whenever x is +ve it will be ONE\n",
    "- whenever x is -ve it will be 0.01\n",
    "- whenever will be using leaky relu fnct there will be chances of vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ab7a7",
   "metadata": {},
   "source": [
    "# ELU - Exponential Linear Unit\n",
    "- whnever x is +ve the output is 1\n",
    "- whnever x is -ve the output is, it will be multiply by some alpha value\n",
    "- overcome the problem of RELU\n",
    "- Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65681ae5",
   "metadata": {},
   "source": [
    "# PRELU\n",
    "- parametric relu\n",
    "- whnever x is +ve the output is 1\n",
    "- whnever x is -ve the output is, it will be multiply by some alpha value\n",
    "- Prelu can solved the problem of RELU and Leaky RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a832cd",
   "metadata": {},
   "source": [
    "# Swish RELU - Self gated\n",
    "- It is used in LSTM\n",
    "- Much computational\n",
    "- It will be used only when HL is more than 40\n",
    "- Zero Centric\n",
    "- solve dead neuron problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74fe06",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "- whenever we have 2 output layers then will be using sigmoid AF(for ex Dog or Cat)\n",
    "- whenever we have more than 2 output layers then will be using Softmax AF (Dog, Cat, Monkey, Animals)\n",
    "- It will used in the Last layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
