{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d28c88",
   "metadata": {},
   "source": [
    "# GD\n",
    "- It takes complete data sets and run for some number of epochs\n",
    "- Epoch - 10K - How mant no of times training the model\n",
    "- Iteration - More than 1 Million, time consuming, more memory, expensive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d02b4",
   "metadata": {},
   "source": [
    "# SGD\n",
    "- It takes one by one record and run for forward and backward propogation\n",
    "- convergence will be more, takes much time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931e0d4",
   "metadata": {},
   "source": [
    "# Mini batch SGD\n",
    "- It takes data in mini batch\n",
    "- For every epoch it takes the batch size \n",
    "- that number of times epoch will take place\n",
    "- convergence will be zig zag which results in noise, coz we are taking batch of data, which will takes wegiht updation time will be much\n",
    "- Less resources\n",
    "- Less computational ecpensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaef947d",
   "metadata": {},
   "source": [
    "# SGD with momentum\n",
    "- Smoothen curve to reach the global minima\n",
    "- Reduce the noise in the data\n",
    "- In order to smoothen the curve will be using exponential weighted average\n",
    "- Arima will be using when we have to do forecasting\n",
    "- Gamma will be 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139c15b",
   "metadata": {},
   "source": [
    "# Adagrad Opti\n",
    "- Different learning rate can be used for each and every neuron for every epoch\n",
    "- DENSE: Most of the features will non zeros\n",
    "- SPARSE: example of bow is sparse - many of features are zeros and some are 1 if using TFIDF it will be in decimals\n",
    "- Initially it will be the bigger Jumps then slower to reach the global minima\n",
    "- disadvantages - as the iteration incrases aplha value also increases which in turns result in smaller LR, there will be much small chnage in new weights to the small weights\n",
    "- As aplha t decreases learning rate increases but our aim is to reduce the error\n",
    "- epsilon will be some +ve number\n",
    "- If alpha t is zero then it will give zero division error for that will be using some +ve value of epsilon\n",
    "- aplha t will be a very big number if NN is deep then LR will be very small then w(old) and w(new) will be same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b8559",
   "metadata": {},
   "source": [
    "# Adadelta and RMSprop\n",
    "- To prevent aplha t increasing to a higher number\n",
    "- Moving average will be used (Exponential weighted average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd5e27",
   "metadata": {},
   "source": [
    "# ADAM\n",
    "- Adaptive moment estimation\n",
    "- Combines two things momentum and RMSprop\n",
    "- with the help of momentum will be getting smoothing\n",
    "- with the help of RMSprom LR will be changed in an efficient maner so that alpha t value also not goes high\n",
    "- For evry GD/optimizer will be using some batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00bc506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
